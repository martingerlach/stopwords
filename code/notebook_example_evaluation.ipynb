{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.3"}},"nbformat_minor":2,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Tutorial for stopword filtering\n\nInteractive notebook for evaluation of stopword lists using topic models.","metadata":{}},{"cell_type":"code","source":"## import packages\n\n%load_ext autoreload\n%autoreload 2\n\nimport os,sys\nimport numpy as np\nimport pandas as pd\n\nimport timeit\nfrom memory_profiler import memory_usage\n\n# display the figure in the notebook\n# %matplotlib inline\n# import matplotlib.pyplot as plt\n# cmap = 'tab10'\n# cm = plt.get_cmap(cmap)\n\n## custom packages\nsrc_dir = os.path.join( 'src')\nsys.path.append(src_dir)\n\nfrom filter_words import run_stopword_statistics\nfrom filter_words import make_stopwords_filter\nfrom filter_words import remove_stopwords_from_list_texts\n\nfrom real_corpora import tranfer_real_corpus_toID_and_shuffle\nfrom ldavb import ldavb_inference_terminal, obtain_ldavb_cpuTime_memory\nfrom evaluation import obtain_nmi_unsup, state_dwz_nmi","metadata":{},"execution_count":1,"outputs":[{"name":"stderr","output_type":"stream","text":"paramiko missing, opening SSH/SCP/SFTP paths will be disabled.  `pip install paramiko` to suppress\n\nSlow version of gensim.models.word2vec is being used\n\nSlow version of gensim.models.doc2vec is being used\n"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 1) Load corpus\n\nGet the 20 newsgroup corpus. These are newsarticles from 20 different categories (newsgroups).\n\nWe get a list of documents, where each entry is a list of tokens","metadata":{}},{"cell_type":"code","source":"corpus_name = '20NewsGroup'\nfilename = os.path.join(os.pardir,'data','%s_corpus.csv'%(corpus_name))\ndf = pd.read_csv(filename,index_col=0)\nlist_texts = [  [h.strip() for h in doc.split()  ] for doc in df['text']    ]\nlist_texts[0] ## this is the first doc","metadata":{"scrolled":false},"execution_count":2,"outputs":[{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"['new',\n\n 'religion',\n\n 'forming',\n\n 'sign',\n\n 'yawn',\n\n 'the',\n\n 'church',\n\n 'kibology',\n\n 'did',\n\n 'first',\n\n 'and',\n\n 'better']"},"metadata":{}}]},{"cell_type":"code","source":"## get topic labels and convert to interger-ids\nlist_topics = [ doc  for doc in df['label']    ]\nlist_topics_unique = list(set(list_topics))\nlist_topics_id = [list_topics_unique.index(i) for i in list_topics]\n\nlist_topics[:10] ## the category labels of the first 10 documents\n# len(list_topics_unique), len(list_topics_id), list_topics_id[:10]","metadata":{},"execution_count":5,"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"['talk_religion_misc',\n\n 'talk_religion_misc',\n\n 'talk_religion_misc',\n\n 'talk_religion_misc',\n\n 'talk_religion_misc',\n\n 'talk_religion_misc',\n\n 'talk_religion_misc',\n\n 'talk_religion_misc',\n\n 'talk_religion_misc',\n\n 'talk_religion_misc']"},"metadata":{}}]},{"cell_type":"markdown","source":"## 2) Get stopword statistics\n\nWe calculate different statistics for each word in order to construct different stopword-filters:\n\n- F, relative frequency\n- I, Information content\n- tfidf, term-frequency-inverse-document-frequency\n- manual, whether the word occurs in the manual stopword list (1), otherwise nan\n\n\n- H, empirical conditional entropy\n- H-tilde, expected conditional entropy from randomized null model\n- N, frequncy (number of counts)\n\n","metadata":{}},{"cell_type":"code","source":"# %%time\n## path to a manual stopword list (this one is from mallet)\npath_stopword_list =  os.path.join(os.pardir,'data','stopword_list_en')\n\n## number of realizations for the random null model\nN_s = 10\n\n## get the statistics\ndf = run_stopword_statistics(list_texts,N_s=N_s,path_stopword_list=path_stopword_list)\n\n## look at the entries\ndf.sort_values(by='F',ascending=False).head()","metadata":{},"execution_count":7,"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/html":"<div>\n\n<style scoped>\n\n    .dataframe tbody tr th:only-of-type {\n\n        vertical-align: middle;\n\n    }\n\n\n\n    .dataframe tbody tr th {\n\n        vertical-align: top;\n\n    }\n\n\n\n    .dataframe thead th {\n\n        text-align: right;\n\n    }\n\n</style>\n\n<table border=\"1\" class=\"dataframe\">\n\n  <thead>\n\n    <tr style=\"text-align: right;\">\n\n      <th></th>\n\n      <th>F</th>\n\n      <th>I</th>\n\n      <th>tfidf</th>\n\n      <th>manual</th>\n\n      <th>H</th>\n\n      <th>H-tilde</th>\n\n      <th>H-tilde_std</th>\n\n      <th>N</th>\n\n    </tr>\n\n  </thead>\n\n  <tbody>\n\n    <tr>\n\n      <th>the</th>\n\n      <td>0.062401</td>\n\n      <td>0.245335</td>\n\n      <td>1.007189</td>\n\n      <td>1.0</td>\n\n      <td>12.982312</td>\n\n      <td>13.227648</td>\n\n      <td>0.003651</td>\n\n      <td>239094</td>\n\n    </tr>\n\n    <tr>\n\n      <th>and</th>\n\n      <td>0.024848</td>\n\n      <td>0.332028</td>\n\n      <td>1.142009</td>\n\n      <td>1.0</td>\n\n      <td>12.800792</td>\n\n      <td>13.132819</td>\n\n      <td>0.007405</td>\n\n      <td>95205</td>\n\n    </tr>\n\n    <tr>\n\n      <th>that</th>\n\n      <td>0.016991</td>\n\n      <td>0.292240</td>\n\n      <td>1.679582</td>\n\n      <td>1.0</td>\n\n      <td>12.764760</td>\n\n      <td>13.057000</td>\n\n      <td>0.006968</td>\n\n      <td>65103</td>\n\n    </tr>\n\n    <tr>\n\n      <th>for</th>\n\n      <td>0.011996</td>\n\n      <td>0.053331</td>\n\n      <td>1.088629</td>\n\n      <td>1.0</td>\n\n      <td>12.916255</td>\n\n      <td>12.969587</td>\n\n      <td>0.008276</td>\n\n      <td>45965</td>\n\n    </tr>\n\n    <tr>\n\n      <th>you</th>\n\n      <td>0.011620</td>\n\n      <td>0.456266</td>\n\n      <td>2.252701</td>\n\n      <td>1.0</td>\n\n      <td>12.497241</td>\n\n      <td>12.953507</td>\n\n      <td>0.008952</td>\n\n      <td>44521</td>\n\n    </tr>\n\n  </tbody>\n\n</table>\n\n</div>","text/plain":"             F         I     tfidf  manual          H    H-tilde  H-tilde_std  \\\n\nthe   0.062401  0.245335  1.007189     1.0  12.982312  13.227648     0.003651   \n\nand   0.024848  0.332028  1.142009     1.0  12.800792  13.132819     0.007405   \n\nthat  0.016991  0.292240  1.679582     1.0  12.764760  13.057000     0.006968   \n\nfor   0.011996  0.053331  1.088629     1.0  12.916255  12.969587     0.008276   \n\nyou   0.011620  0.456266  2.252701     1.0  12.497241  12.953507     0.008952   \n\n\n\n           N  \n\nthe   239094  \n\nand    95205  \n\nthat   65103  \n\nfor    45965  \n\nyou    44521  "},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{"scrolled":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 3)  Construct a stopword filter\n\nWe construct different stopword filters based in different statistics.\n\nFor this we have to specify 3 different components:\n\n- A) method; this specifies the statistic that we use to construct the stopword list. In detail, we define a statistic $S(w)$ and assign words to the stopword list starting from the low-to-high (e.g. $S(w) = F(w)$ assign low-frequency words to the stopword list). Possible options are:\n\n    - 'INFOR',  filter words with high values of Information-content I [S=-I]\n    - 'BOTTOM', filter words with low values of frequency [S = F]\n    - 'TOP', filter words with high values of frequency [S = 1/F]\n    - 'TFIDF', filter words with low values of tfidf [S=tfidf]\n    - 'TFIDF_r', filter words with high values of tfidf [S=-tfidf]\n    - 'MANUAL', filter words from manual stopword list; supply path via path_stopword_list (S = 1 if word is in the list, else it is nan, i.e. cannot be considered for removal.\n        \n        \n- B) cutoff_type [defines the way in which we choose the cutoff]\n\n     - 'p', selects stopword list such that a fraction p of tokens gets removed (approximately)\n     - 'n', selects stopword list such that a number n of types gets removed\n     - 't', selects stopword list such that all words with S<=S_t get removed\n    \n    \n \n- C) cutoff_val [defines the value on which to do the thresholding, see cutoff_type for details]\n\n\n\nBelow you can select different options and inspect the result.\n\nThe resulting dataframe ```df_filter``` contains the words that were assigned to the stopword list based on the selection criteria.","metadata":{}},{"cell_type":"code","source":"## method-options\n# method = 'INFOR'\n# method = 'BOTTOM'\n# method = 'TOP'\n# method = 'TFIDF'\n# method = 'TFIDF_r'\nmethod = 'MANUAL'\n\n\n\n## remove fraction of tokens\ncutoff_type = 'p'\ncutoff_val = 0.4\n\n## remove number of types\n# cutoff_type = 'n'\n# cutoff_val = 10\n\n## remove above a threshold value\n# cutoff_type = 't'\n# cutoff_val = 1\n\ndf_filter = make_stopwords_filter(df,\n                                  method = method,\n                                  cutoff_type = cutoff_type, \n                                  cutoff_val = cutoff_val, )","metadata":{},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"df_filter","metadata":{},"execution_count":9,"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/html":"<div>\n\n<style scoped>\n\n    .dataframe tbody tr th:only-of-type {\n\n        vertical-align: middle;\n\n    }\n\n\n\n    .dataframe tbody tr th {\n\n        vertical-align: top;\n\n    }\n\n\n\n    .dataframe thead th {\n\n        text-align: right;\n\n    }\n\n</style>\n\n<table border=\"1\" class=\"dataframe\">\n\n  <thead>\n\n    <tr style=\"text-align: right;\">\n\n      <th></th>\n\n      <th>F-cumsum</th>\n\n      <th>S</th>\n\n    </tr>\n\n  </thead>\n\n  <tbody>\n\n    <tr>\n\n      <th>able</th>\n\n      <td>0.000443</td>\n\n      <td>1.0</td>\n\n    </tr>\n\n    <tr>\n\n      <th>about</th>\n\n      <td>0.004027</td>\n\n      <td>1.0</td>\n\n    </tr>\n\n    <tr>\n\n      <th>above</th>\n\n      <td>0.004478</td>\n\n      <td>1.0</td>\n\n    </tr>\n\n    <tr>\n\n      <th>according</th>\n\n      <td>0.004690</td>\n\n      <td>1.0</td>\n\n    </tr>\n\n    <tr>\n\n      <th>accordingly</th>\n\n      <td>0.004695</td>\n\n      <td>1.0</td>\n\n    </tr>\n\n    <tr>\n\n      <th>across</th>\n\n      <td>0.004820</td>\n\n      <td>1.0</td>\n\n    </tr>\n\n    <tr>\n\n      <th>actually</th>\n\n      <td>0.005356</td>\n\n      <td>1.0</td>\n\n    </tr>\n\n    <tr>\n\n      <th>after</th>\n\n      <td>0.006414</td>\n\n      <td>1.0</td>\n\n    </tr>\n\n    <tr>\n\n      <th>afterwards</th>\n\n      <td>0.006432</td>\n\n      <td>1.0</td>\n\n    </tr>\n\n    <tr>\n\n      <th>again</th>\n\n      <td>0.007023</td>\n\n      <td>1.0</td>\n\n    </tr>\n\n    <tr>\n\n      <th>against</th>\n\n      <td>0.007619</td>\n\n      <td>1.0</td>\n\n    </tr>\n\n    <tr>\n\n      <th>all</th>\n\n      <td>0.011616</td>\n\n      <td>1.0</td>\n\n    </tr>\n\n    <tr>\n\n      <th>allow</th>\n\n      <td>0.011796</td>\n\n      <td>1.0</td>\n\n    </tr>\n\n    <tr>\n\n      <th>allows</th>\n\n      <td>0.011898</td>\n\n      <td>1.0</td>\n\n    </tr>\n\n    <tr>\n\n      <th>almost</th>\n\n      <td>0.012186</td>\n\n      <td>1.0</td>\n\n    </tr>\n\n    <tr>\n\n      <th>alone</th>\n\n      <td>0.012315</td>\n\n      <td>1.0</td>\n\n    </tr>\n\n    <tr>\n\n      <th>along</th>\n\n      <td>0.012493</td>\n\n      <td>1.0</td>\n\n    </tr>\n\n    <tr>\n\n      <th>already</th>\n\n      <td>0.012843</td>\n\n      <td>1.0</td>\n\n    </tr>\n\n    <tr>\n\n      <th>also</th>\n\n      <td>0.014734</td>\n\n      <td>1.0</td>\n\n    </tr>\n\n    <tr>\n\n      <th>although</th>\n\n      <td>0.015005</td>\n\n      <td>1.0</td>\n\n    </tr>\n\n    <tr>\n\n      <th>always</th>\n\n      <td>0.015440</td>\n\n      <td>1.0</td>\n\n    </tr>\n\n    <tr>\n\n      <th>among</th>\n\n      <td>0.015668</td>\n\n      <td>1.0</td>\n\n    </tr>\n\n    <tr>\n\n      <th>amongst</th>\n\n      <td>0.015688</td>\n\n      <td>1.0</td>\n\n    </tr>\n\n    <tr>\n\n      <th>and</th>\n\n      <td>0.040536</td>\n\n      <td>1.0</td>\n\n    </tr>\n\n    <tr>\n\n      <th>another</th>\n\n      <td>0.041252</td>\n\n      <td>1.0</td>\n\n    </tr>\n\n    <tr>\n\n      <th>any</th>\n\n      <td>0.044267</td>\n\n      <td>1.0</td>\n\n    </tr>\n\n    <tr>\n\n      <th>anybody</th>\n\n      <td>0.044529</td>\n\n      <td>1.0</td>\n\n    </tr>\n\n    <tr>\n\n      <th>anyhow</th>\n\n      <td>0.044539</td>\n\n      <td>1.0</td>\n\n    </tr>\n\n    <tr>\n\n      <th>anyone</th>\n\n      <td>0.045635</td>\n\n      <td>1.0</td>\n\n    </tr>\n\n    <tr>\n\n      <th>anything</th>\n\n      <td>0.046247</td>\n\n      <td>1.0</td>\n\n    </tr>\n\n    <tr>\n\n      <th>...</th>\n\n      <td>...</td>\n\n      <td>...</td>\n\n    </tr>\n\n    <tr>\n\n      <th>viz</th>\n\n      <td>0.369992</td>\n\n      <td>1.0</td>\n\n    </tr>\n\n    <tr>\n\n      <th>want</th>\n\n      <td>0.371084</td>\n\n      <td>1.0</td>\n\n    </tr>\n\n    <tr>\n\n      <th>wants</th>\n\n      <td>0.371262</td>\n\n      <td>1.0</td>\n\n    </tr>\n\n    <tr>\n\n      <th>was</th>\n\n      <td>0.377195</td>\n\n      <td>1.0</td>\n\n    </tr>\n\n    <tr>\n\n      <th>way</th>\n\n      <td>0.378502</td>\n\n      <td>1.0</td>\n\n    </tr>\n\n    <tr>\n\n      <th>welcome</th>\n\n      <td>0.378588</td>\n\n      <td>1.0</td>\n\n    </tr>\n\n    <tr>\n\n      <th>well</th>\n\n      <td>0.380067</td>\n\n      <td>1.0</td>\n\n    </tr>\n\n    <tr>\n\n      <th>went</th>\n\n      <td>0.380366</td>\n\n      <td>1.0</td>\n\n    </tr>\n\n    <tr>\n\n      <th>were</th>\n\n      <td>0.382771</td>\n\n      <td>1.0</td>\n\n    </tr>\n\n    <tr>\n\n      <th>what</th>\n\n      <td>0.387054</td>\n\n      <td>1.0</td>\n\n    </tr>\n\n    <tr>\n\n      <th>whatever</th>\n\n      <td>0.387270</td>\n\n      <td>1.0</td>\n\n    </tr>\n\n    <tr>\n\n      <th>when</th>\n\n      <td>0.389719</td>\n\n      <td>1.0</td>\n\n    </tr>\n\n    <tr>\n\n      <th>whence</th>\n\n      <td>0.389721</td>\n\n      <td>1.0</td>\n\n    </tr>\n\n    <tr>\n\n      <th>whenever</th>\n\n      <td>0.389768</td>\n\n      <td>1.0</td>\n\n    </tr>\n\n    <tr>\n\n      <th>where</th>\n\n      <td>0.390902</td>\n\n      <td>1.0</td>\n\n    </tr>\n\n    <tr>\n\n      <th>whereas</th>\n\n      <td>0.390936</td>\n\n      <td>1.0</td>\n\n    </tr>\n\n    <tr>\n\n      <th>whereby</th>\n\n      <td>0.390946</td>\n\n      <td>1.0</td>\n\n    </tr>\n\n    <tr>\n\n      <th>wherein</th>\n\n      <td>0.390951</td>\n\n      <td>1.0</td>\n\n    </tr>\n\n    <tr>\n\n      <th>whereupon</th>\n\n      <td>0.390953</td>\n\n      <td>1.0</td>\n\n    </tr>\n\n    <tr>\n\n      <th>wherever</th>\n\n      <td>0.390968</td>\n\n      <td>1.0</td>\n\n    </tr>\n\n    <tr>\n\n      <th>whether</th>\n\n      <td>0.391364</td>\n\n      <td>1.0</td>\n\n    </tr>\n\n    <tr>\n\n      <th>which</th>\n\n      <td>0.394027</td>\n\n      <td>1.0</td>\n\n    </tr>\n\n    <tr>\n\n      <th>while</th>\n\n      <td>0.394793</td>\n\n      <td>1.0</td>\n\n    </tr>\n\n    <tr>\n\n      <th>whither</th>\n\n      <td>0.394794</td>\n\n      <td>1.0</td>\n\n    </tr>\n\n    <tr>\n\n      <th>who</th>\n\n      <td>0.397638</td>\n\n      <td>1.0</td>\n\n    </tr>\n\n    <tr>\n\n      <th>whoever</th>\n\n      <td>0.397673</td>\n\n      <td>1.0</td>\n\n    </tr>\n\n    <tr>\n\n      <th>whole</th>\n\n      <td>0.397988</td>\n\n      <td>1.0</td>\n\n    </tr>\n\n    <tr>\n\n      <th>whom</th>\n\n      <td>0.398077</td>\n\n      <td>1.0</td>\n\n    </tr>\n\n    <tr>\n\n      <th>whose</th>\n\n      <td>0.398216</td>\n\n      <td>1.0</td>\n\n    </tr>\n\n    <tr>\n\n      <th>why</th>\n\n      <td>0.399567</td>\n\n      <td>1.0</td>\n\n    </tr>\n\n  </tbody>\n\n</table>\n\n<p>437 rows × 2 columns</p>\n\n</div>","text/plain":"             F-cumsum    S\n\nable         0.000443  1.0\n\nabout        0.004027  1.0\n\nabove        0.004478  1.0\n\naccording    0.004690  1.0\n\naccordingly  0.004695  1.0\n\nacross       0.004820  1.0\n\nactually     0.005356  1.0\n\nafter        0.006414  1.0\n\nafterwards   0.006432  1.0\n\nagain        0.007023  1.0\n\nagainst      0.007619  1.0\n\nall          0.011616  1.0\n\nallow        0.011796  1.0\n\nallows       0.011898  1.0\n\nalmost       0.012186  1.0\n\nalone        0.012315  1.0\n\nalong        0.012493  1.0\n\nalready      0.012843  1.0\n\nalso         0.014734  1.0\n\nalthough     0.015005  1.0\n\nalways       0.015440  1.0\n\namong        0.015668  1.0\n\namongst      0.015688  1.0\n\nand          0.040536  1.0\n\nanother      0.041252  1.0\n\nany          0.044267  1.0\n\nanybody      0.044529  1.0\n\nanyhow       0.044539  1.0\n\nanyone       0.045635  1.0\n\nanything     0.046247  1.0\n\n...               ...  ...\n\nviz          0.369992  1.0\n\nwant         0.371084  1.0\n\nwants        0.371262  1.0\n\nwas          0.377195  1.0\n\nway          0.378502  1.0\n\nwelcome      0.378588  1.0\n\nwell         0.380067  1.0\n\nwent         0.380366  1.0\n\nwere         0.382771  1.0\n\nwhat         0.387054  1.0\n\nwhatever     0.387270  1.0\n\nwhen         0.389719  1.0\n\nwhence       0.389721  1.0\n\nwhenever     0.389768  1.0\n\nwhere        0.390902  1.0\n\nwhereas      0.390936  1.0\n\nwhereby      0.390946  1.0\n\nwherein      0.390951  1.0\n\nwhereupon    0.390953  1.0\n\nwherever     0.390968  1.0\n\nwhether      0.391364  1.0\n\nwhich        0.394027  1.0\n\nwhile        0.394793  1.0\n\nwhither      0.394794  1.0\n\nwho          0.397638  1.0\n\nwhoever      0.397673  1.0\n\nwhole        0.397988  1.0\n\nwhom         0.398077  1.0\n\nwhose        0.398216  1.0\n\nwhy          0.399567  1.0\n\n\n\n[437 rows x 2 columns]"},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 4) Apply the stopword-filter to remove the words from the list of texts\n\nWe inspect one particular document for the effect of the stopword filter.\n\nWe report the remaining faction of tokens in the filtered list of texts.","metadata":{}},{"cell_type":"code","source":"## get the list of words from df_filter and get a filtered list_of_texts\nlist_words_filter = list(df_filter.index)\nlist_texts_filter = remove_stopwords_from_list_texts(list_texts, list_words_filter)\n\nprint('Original text:', list_texts[0])\nprint('Filtered text:', list_texts_filter[0])\nN = sum([ len(doc) for doc in list_texts ])\nN_filter = sum([ len(doc) for doc in list_texts_filter ])\nprint('Remaining fraction of tokens',N_filter/N)","metadata":{},"execution_count":10,"outputs":[{"name":"stdout","output_type":"stream","text":"Original text: ['new', 'religion', 'forming', 'sign', 'yawn', 'the', 'church', 'kibology', 'did', 'first', 'and', 'better']\n\nFiltered text: ['religion', 'forming', 'sign', 'yawn', 'church', 'kibology']\n\nRemaining fraction of tokens 0.6004331396175813\n"}]},{"cell_type":"code","source":"# len(list_texts), len(list_texts_filter)","metadata":{"scrolled":true},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"Note that:\nIn our work, if there are empty documents after stopword removal, we will remove the empty documents and randomly assign a category to these documents during the document classification evaluation task.","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 5) Run topic modeling algorithm: ldavb & evaluation\n\nAfter running topic modleing algorithm, we retrieve the following metrics for its performance:\n\n- Accuracy\n\n- Reproducibility\n\n- Time and memory\n\n- Coherence\n\nNote that the results for the LDAVB topic model algorithm shown here is reported in Supplementary Figures 8 A and B.","metadata":{}},{"cell_type":"code","source":"# input_k is the assumed number of topics for LDAVB\ninput_k = 20","metadata":{},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"# %%time\n# Run the topic model\nshuffle_texts_list, shuffle_topic_list = tranfer_real_corpus_toID_and_shuffle(list_texts_filter, list_topics_id)\ndict_output_topicModel = ldavb_inference_terminal(shuffle_texts_list, input_k, flag_coherence=1)","metadata":{},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"# output results of ldavb\ndict_output_topicModel.keys()","metadata":{},"execution_count":14,"outputs":[{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"dict_keys(['p_td_infer', 'p_wt_infer', 'coherence', 'state_dwz_infer'])"},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Metric 1: Accuracy\n\nWe measure accuracy by calculating the Normalized mutual information between each document's category label assigned from the inferred topic model (the most likely topic) and the metadata's category-label. ","metadata":{}},{"cell_type":"code","source":"## the inferred topic distribution for each document\np_td_infer = dict_output_topicModel['p_td_infer']\n\n## we compare the topic distribution with the category labels\nunsupervised_classification_nmi = obtain_nmi_unsup(shuffle_topic_list, p_td_infer)\nunsupervised_classification_nmi","metadata":{},"execution_count":16,"outputs":[{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"0.3962771831426493"},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\n### Metric 2: reproducibility\n\nWe measure reproducibility by comparing the solutions of two runs of the same topic model. Thus, we re-run the inference and compare with the previous inference result.","metadata":{}},{"cell_type":"code","source":"# %%time \ndict_output_topicModel_2 = ldavb_inference_terminal(shuffle_texts_list, input_k, flag_coherence=1)","metadata":{},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"## topic-labels for each word token in 1st run\nstate_dwz_infer = dict_output_topicModel['state_dwz_infer']\n\n## topic-labels for each word token in 2nd run\nstate_dwz_infer_2 = dict_output_topicModel_2['state_dwz_infer']","metadata":{},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"## compare topic labels from 1st and 2nd run\nreproducibility_final = state_dwz_nmi(state_dwz_infer, state_dwz_infer_2, input_k, input_k)\nreproducibility_final","metadata":{},"execution_count":16,"outputs":[{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"0.1834962449776976"},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Additional metrics","metadata":{}},{"cell_type":"markdown","source":"### get the computational time and memory used in the process","metadata":{}},{"cell_type":"code","source":"# %%time\nelapsed_time, increment_memory = obtain_ldavb_cpuTime_memory(shuffle_texts_list, input_k)","metadata":{},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"elapsed_time, increment_memory","metadata":{},"execution_count":18,"outputs":[{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"(33.04066830701777, 20.0)"},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### get the mean coherence over all topics","metadata":{}},{"cell_type":"code","source":"coherence_array = dict_output_topicModel['coherence']\ncoherence_mean = coherence_array.mean()\ncoherence_mean","metadata":{},"execution_count":19,"outputs":[{"execution_count":19,"output_type":"execute_result","data":{"text/plain":"-1.9597395901826817"},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## end","metadata":{}},{"cell_type":"code","source":"1","metadata":{},"execution_count":20,"outputs":[{"execution_count":20,"output_type":"execute_result","data":{"text/plain":"1"},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}