{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial for stopword filtering\n",
    "\n",
    "Interactive notebook for demonstrating filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## import packages\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os,sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import timeit\n",
    "from memory_profiler import memory_usage\n",
    "\n",
    "# display the figure in the notebook\n",
    "# %matplotlib inline\n",
    "# import matplotlib.pyplot as plt\n",
    "# cmap = 'tab10'\n",
    "# cm = plt.get_cmap(cmap)\n",
    "\n",
    "## custom packages\n",
    "src_dir = os.path.join( 'src')\n",
    "sys.path.append(src_dir)\n",
    "\n",
    "from filter_words import run_stopword_statistics\n",
    "from filter_words import make_stopwords_filter\n",
    "from filter_words import remove_stopwords_from_list_texts\n",
    "\n",
    "from real_corpora import tranfer_real_corpus_toID_and_shuffle\n",
    "from ldavb import ldavb_inference_terminal, obtain_ldavb_cpuTime_memory\n",
    "from evaluation import obtain_nmi_unsup, state_dwz_nmi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Load corpus\n",
    "\n",
    "Get the 20 newsgroup corpus. These are newsarticles from 20 different categories (newsgroups).\n",
    "\n",
    "We get a list of documents, where each entry is a list of tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['new',\n",
       " 'religion',\n",
       " 'forming',\n",
       " 'sign',\n",
       " 'yawn',\n",
       " 'the',\n",
       " 'church',\n",
       " 'kibology',\n",
       " 'did',\n",
       " 'first',\n",
       " 'and',\n",
       " 'better']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_name = '20NewsGroup'\n",
    "filename = os.path.join(os.pardir,'data','%s_corpus.csv'%(corpus_name))\n",
    "df = pd.read_csv(filename,index_col=0)\n",
    "list_texts = [  [h.strip() for h in doc.split()  ] for doc in df['text']    ]\n",
    "list_texts[0] ## this is the first doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['talk_religion_misc',\n",
       " 'talk_religion_misc',\n",
       " 'talk_religion_misc',\n",
       " 'talk_religion_misc',\n",
       " 'talk_religion_misc',\n",
       " 'talk_religion_misc',\n",
       " 'talk_religion_misc',\n",
       " 'talk_religion_misc',\n",
       " 'talk_religion_misc',\n",
       " 'talk_religion_misc']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_topics = [ doc  for doc in df['label']    ]\n",
    "list_topics[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20, 18803, [18, 18, 18, 18, 18, 18, 18, 18, 18, 18])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_topics_unique = list(set(list_topics))\n",
    "list_topics_id = [list_topics_unique.index(i) for i in list_topics]\n",
    "\n",
    "len(list_topics_unique), len(list_topics_id), list_topics_id[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Get stopword statistics\n",
    "\n",
    "We calculate different statistics for each word in order to construct different stopword-filters:\n",
    "\n",
    "- F, relative frequency\n",
    "- I, Information content\n",
    "- tfidf, term-frequency-inverse-document-frequency\n",
    "- manual, whether the word occurs in the manual stopword list (1), otherwise nan\n",
    "\n",
    "\n",
    "- H, empirical conditional entropy\n",
    "- H-tilde, expected conditional entropy from randomized null model\n",
    "- N, frequncy (number of counts)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 11.2 s, sys: 936 ms, total: 12.1 s\n",
      "Wall time: 12.2 s\n"
     ]
    }
   ],
   "source": [
    "# %%time\n",
    "## path to a manual stopword list (this one is from mallet)\n",
    "path_stopword_list =  os.path.join(os.pardir,'data','stopword_list_en')\n",
    "\n",
    "## number of realizations for the random null model\n",
    "N_s = 2\n",
    "\n",
    "## get the statistics\n",
    "df = run_stopword_statistics(list_texts,N_s=N_s,path_stopword_list=path_stopword_list)\n",
    "\n",
    "## look at the entries\n",
    "df.sort_values(by='F',ascending=False).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3)  Construct a stopword filter\n",
    "\n",
    "We construct different stopword filters based in different statistics.\n",
    "\n",
    "For this we have to specify 3 different components:\n",
    "\n",
    "- A) method; this specifies the statistic that we use to construct the stopword list. In detail, we define a statistic $S(w)$ and assign words to the stopword list starting from the low-to-high (e.g. $S(w) = F(w)$ assign low-frequency words to the stopword list). Possible options are:\n",
    "\n",
    "    - 'INFOR',  filter words with high values of Information-content I [S=-I]\n",
    "    - 'BOTTOM', filter words with low values of frequency [S = F]\n",
    "    - 'TOP', filter words with high values of frequency [S = 1/F]\n",
    "    - 'TFIDF', filter words with low values of tfidf [S=tfidf]\n",
    "    - 'TFIDF_r', filter words with high values of tfidf [S=-tfidf]\n",
    "    - 'MANUAL', filter words from manual stopword list; supply path via path_stopword_list (S = 1 if word is in the list, else it is nan, i.e. cannot be considered for removal.\n",
    "        \n",
    "        \n",
    "- B) cutoff_type [defines the way in which we choose the cutoff]\n",
    "\n",
    "     - 'p', selects stopword list such that a fraction p of tokens gets removed (approximately)\n",
    "     - 'n', selects stopword list such that a number n of types gets removed\n",
    "     - 't', selects stopword list such that all words with S<=S_t get removed\n",
    "    \n",
    "    \n",
    " \n",
    "- C) cutoff_val [defines the value on which to do the thresholding, see cutoff_type for details]\n",
    "\n",
    "\n",
    "\n",
    "Below you can select different options and inspect the result.\n",
    "\n",
    "The resulting dataframe ```df_filter``` contains the words that were assigned to the stopword list based on the selection criteria."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## method-options\n",
    "# method = 'INFOR'\n",
    "# method = 'BOTTOM'\n",
    "# method = 'TOP'\n",
    "# method = 'TFIDF'\n",
    "# method = 'TFIDF_r'\n",
    "method = 'MANUAL'\n",
    "\n",
    "\n",
    "\n",
    "## remove fraction of tokens\n",
    "cutoff_type = 'p'\n",
    "cutoff_val = 0.4\n",
    "\n",
    "## remove number of types\n",
    "# cutoff_type = 'n'\n",
    "# cutoff_val = 10\n",
    "\n",
    "## remove above a threshold value\n",
    "# cutoff_type = 't'\n",
    "# cutoff_val = 1\n",
    "\n",
    "df_filter = make_stopwords_filter(df,\n",
    "                                  method = method,\n",
    "                                  cutoff_type = cutoff_type, \n",
    "                                  cutoff_val = cutoff_val, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>F-cumsum</th>\n",
       "      <th>S</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>able</th>\n",
       "      <td>0.000443</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>about</th>\n",
       "      <td>0.004027</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>above</th>\n",
       "      <td>0.004478</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>according</th>\n",
       "      <td>0.004690</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>accordingly</th>\n",
       "      <td>0.004695</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>across</th>\n",
       "      <td>0.004820</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>actually</th>\n",
       "      <td>0.005356</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>after</th>\n",
       "      <td>0.006414</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>afterwards</th>\n",
       "      <td>0.006432</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>again</th>\n",
       "      <td>0.007023</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>against</th>\n",
       "      <td>0.007619</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>all</th>\n",
       "      <td>0.011616</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>allow</th>\n",
       "      <td>0.011796</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>allows</th>\n",
       "      <td>0.011898</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>almost</th>\n",
       "      <td>0.012186</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>alone</th>\n",
       "      <td>0.012315</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>along</th>\n",
       "      <td>0.012493</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>already</th>\n",
       "      <td>0.012843</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>also</th>\n",
       "      <td>0.014734</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>although</th>\n",
       "      <td>0.015005</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>always</th>\n",
       "      <td>0.015440</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>among</th>\n",
       "      <td>0.015668</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>amongst</th>\n",
       "      <td>0.015688</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>and</th>\n",
       "      <td>0.040536</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>another</th>\n",
       "      <td>0.041252</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>any</th>\n",
       "      <td>0.044267</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>anybody</th>\n",
       "      <td>0.044529</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>anyhow</th>\n",
       "      <td>0.044539</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>anyone</th>\n",
       "      <td>0.045635</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>anything</th>\n",
       "      <td>0.046247</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>viz</th>\n",
       "      <td>0.369992</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>want</th>\n",
       "      <td>0.371084</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wants</th>\n",
       "      <td>0.371262</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>was</th>\n",
       "      <td>0.377195</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>way</th>\n",
       "      <td>0.378502</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>welcome</th>\n",
       "      <td>0.378588</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>well</th>\n",
       "      <td>0.380067</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>went</th>\n",
       "      <td>0.380366</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>were</th>\n",
       "      <td>0.382771</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>what</th>\n",
       "      <td>0.387054</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>whatever</th>\n",
       "      <td>0.387270</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>when</th>\n",
       "      <td>0.389719</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>whence</th>\n",
       "      <td>0.389721</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>whenever</th>\n",
       "      <td>0.389768</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>where</th>\n",
       "      <td>0.390902</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>whereas</th>\n",
       "      <td>0.390936</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>whereby</th>\n",
       "      <td>0.390946</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wherein</th>\n",
       "      <td>0.390951</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>whereupon</th>\n",
       "      <td>0.390953</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wherever</th>\n",
       "      <td>0.390968</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>whether</th>\n",
       "      <td>0.391364</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>which</th>\n",
       "      <td>0.394027</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>while</th>\n",
       "      <td>0.394793</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>whither</th>\n",
       "      <td>0.394794</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>who</th>\n",
       "      <td>0.397638</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>whoever</th>\n",
       "      <td>0.397673</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>whole</th>\n",
       "      <td>0.397988</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>whom</th>\n",
       "      <td>0.398077</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>whose</th>\n",
       "      <td>0.398216</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>why</th>\n",
       "      <td>0.399567</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>437 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             F-cumsum    S\n",
       "able         0.000443  1.0\n",
       "about        0.004027  1.0\n",
       "above        0.004478  1.0\n",
       "according    0.004690  1.0\n",
       "accordingly  0.004695  1.0\n",
       "across       0.004820  1.0\n",
       "actually     0.005356  1.0\n",
       "after        0.006414  1.0\n",
       "afterwards   0.006432  1.0\n",
       "again        0.007023  1.0\n",
       "against      0.007619  1.0\n",
       "all          0.011616  1.0\n",
       "allow        0.011796  1.0\n",
       "allows       0.011898  1.0\n",
       "almost       0.012186  1.0\n",
       "alone        0.012315  1.0\n",
       "along        0.012493  1.0\n",
       "already      0.012843  1.0\n",
       "also         0.014734  1.0\n",
       "although     0.015005  1.0\n",
       "always       0.015440  1.0\n",
       "among        0.015668  1.0\n",
       "amongst      0.015688  1.0\n",
       "and          0.040536  1.0\n",
       "another      0.041252  1.0\n",
       "any          0.044267  1.0\n",
       "anybody      0.044529  1.0\n",
       "anyhow       0.044539  1.0\n",
       "anyone       0.045635  1.0\n",
       "anything     0.046247  1.0\n",
       "...               ...  ...\n",
       "viz          0.369992  1.0\n",
       "want         0.371084  1.0\n",
       "wants        0.371262  1.0\n",
       "was          0.377195  1.0\n",
       "way          0.378502  1.0\n",
       "welcome      0.378588  1.0\n",
       "well         0.380067  1.0\n",
       "went         0.380366  1.0\n",
       "were         0.382771  1.0\n",
       "what         0.387054  1.0\n",
       "whatever     0.387270  1.0\n",
       "when         0.389719  1.0\n",
       "whence       0.389721  1.0\n",
       "whenever     0.389768  1.0\n",
       "where        0.390902  1.0\n",
       "whereas      0.390936  1.0\n",
       "whereby      0.390946  1.0\n",
       "wherein      0.390951  1.0\n",
       "whereupon    0.390953  1.0\n",
       "wherever     0.390968  1.0\n",
       "whether      0.391364  1.0\n",
       "which        0.394027  1.0\n",
       "while        0.394793  1.0\n",
       "whither      0.394794  1.0\n",
       "who          0.397638  1.0\n",
       "whoever      0.397673  1.0\n",
       "whole        0.397988  1.0\n",
       "whom         0.398077  1.0\n",
       "whose        0.398216  1.0\n",
       "why          0.399567  1.0\n",
       "\n",
       "[437 rows x 2 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4) Apply the stopword-filter to remove the words from the list of texts\n",
    "\n",
    "We inspect one particular document for the effect of the stopword filter.\n",
    "\n",
    "We report the remaining faction of tokens in the filtered list of texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text: ['new', 'religion', 'forming', 'sign', 'yawn', 'the', 'church', 'kibology', 'did', 'first', 'and', 'better']\n",
      "Filtered text: ['religion', 'forming', 'sign', 'yawn', 'church', 'kibology']\n",
      "Remaining fraction of tokens 0.6004331396175813\n"
     ]
    }
   ],
   "source": [
    "## get the list of words from df_filter and get a filtered list_of_texts\n",
    "list_words_filter = list(df_filter.index)\n",
    "list_texts_filter = remove_stopwords_from_list_texts(list_texts, list_words_filter)\n",
    "\n",
    "print('Original text:', list_texts[0])\n",
    "print('Filtered text:', list_texts_filter[0])\n",
    "N = sum([ len(doc) for doc in list_texts ])\n",
    "N_filter = sum([ len(doc) for doc in list_texts_filter ])\n",
    "print('Remaining fraction of tokens',N_filter/N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18803, 18803)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list_texts), len(list_texts_filter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Note that:\n",
    "In our work, if there are empty documents after stopword removal, we will remove the empty documents and randomly assign a category to these documents during the document classification evaluation task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5) Run topic modeling algorithm: ldavb & evaluation\n",
    "\n",
    "After running topic modleing algorithm, we retrieve the following metrics for its performance:\n",
    "\n",
    "- Accuracy\n",
    "\n",
    "- Reproducibility\n",
    "\n",
    "- Time and memory\n",
    "\n",
    "- Coherence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# input_k is the assumed number of topics for LDAVB\n",
    "input_k = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4min 28s, sys: 8.88 s, total: 4min 37s\n",
      "Wall time: 4min 38s\n"
     ]
    }
   ],
   "source": [
    "# %%time\n",
    "# Run the topic model\n",
    "shuffle_texts_list, shuffle_topic_list = tranfer_real_corpus_toID_and_shuffle(list_texts_filter, list_topics_id)\n",
    "dict_output_topicModel = ldavb_inference_terminal(shuffle_texts_list, input_k, flag_coherence=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['coherence', 'state_dwz_infer', 'p_td_infer', 'p_wt_infer'])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# output results of ldavb\n",
    "dict_output_topicModel.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metric 1: Accuracy\n",
    "\n",
    "We measure accuracy by NMI for unsupervised classicification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.38742110461676693"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_td_infer = dict_output_topicModel['p_td_infer']\n",
    "unsupervised_classification_nmi = obtain_nmi_unsup(shuffle_topic_list, p_td_infer)\n",
    "unsupervised_classification_nmi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Metric 2: reproducibility\n",
    "\n",
    "re-run the topic modeling algorithm to obtain the reproducibility\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4min 14s, sys: 5.81 s, total: 4min 20s\n",
      "Wall time: 4min 22s\n"
     ]
    }
   ],
   "source": [
    "# %%time \n",
    "dict_output_topicModel_2 = ldavb_inference_terminal(shuffle_texts_list, input_k, flag_coherence=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "state_dwz_infer = dict_output_topicModel['state_dwz_infer']\n",
    "state_dwz_infer_2 = dict_output_topicModel_2['state_dwz_infer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.20002348483102181"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reproducibility_final = state_dwz_nmi(state_dwz_infer, state_dwz_infer_2, input_k, input_k)\n",
    "reproducibility_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get the computational time and memory used in the process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 1s, sys: 868 ms, total: 1min 1s\n",
      "Wall time: 1min 2s\n"
     ]
    }
   ],
   "source": [
    "# %%time\n",
    "elapsed_time, increment_memory = obtain_ldavb_cpuTime_memory(shuffle_texts_list, input_k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(62.1458009009948, 63.078125)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "elapsed_time, increment_memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get the mean coherence over all topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1.8657491228326841"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coherence_array = dict_output_topicModel['coherence']\n",
    "coherence_mean = coherence_array.mean()\n",
    "coherence_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
